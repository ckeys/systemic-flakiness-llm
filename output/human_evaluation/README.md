# Human Evaluation for LLM-as-Judge Validation

## Purpose
This evaluation validates the reliability of LLM-based semantic similarity scoring by comparing LLM scores with human judgments.

## Evaluation Task
For each sample, you will see:
1. **LLM Diagnosis (A)**: The diagnosis generated by our method
2. **Human Ground Truth (B)**: The expert-annotated root cause description

Your task: Rate the semantic similarity between A and B using the scale below.

## Scoring Rubric (1-5 Scale)

| Score | Description | Criteria |
|-------|-------------|----------|
| 1 | Completely different | Different root causes, no meaningful overlap |
| 2 | Related but different | Same general area but different specific causes |
| 3 | Same category, different details | Same root cause category, but different technical details |
| 4 | Same root cause, minor differences | Essentially the same diagnosis with wording variations |
| 5 | Essentially identical | Same root cause with same level of detail |

## Instructions

1. Read both diagnoses carefully
2. Focus on the **semantic meaning**, not exact wording
3. Consider: Do both diagnoses point to the same underlying technical issue?
4. Assign a score from 1-5 based on the rubric above
5. (Optional) Note if Diagnosis A is "Correct", "Partial", or "Incorrect"

## Important Notes

- Do NOT look at the LLM's scores before completing your evaluation
- Rate each sample independently
- If unsure, lean toward the middle (score 3)
- Focus on root cause identification, not symptom description

## Files

- `evaluation_sheet.csv`: Main evaluation spreadsheet (fill in Human_Score column)
- `evaluation_details.md`: Detailed view of all samples with full text
- `llm_scores_hidden.json`: LLM scores (DO NOT OPEN until evaluation is complete)

## After Evaluation

Run the analysis script to compute agreement metrics:
```bash
python analyze_human_evaluation.py
```
